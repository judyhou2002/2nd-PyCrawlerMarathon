{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies = {'over18':'1'})\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    #內文\n",
    "    main_content = soup.find(id='main-content')\n",
    "\n",
    "    #作者、標題、發文時間\n",
    "    metas = main_content.find_all('div', class_='article-metaline')\n",
    "\n",
    "    author = metas[0].find('span', class_='article-meta-value').text.strip()\n",
    "    title = metas[1].find('span', class_='article-meta-value').text.strip()\n",
    "    date = metas[2].find('span', class_='article-meta-value').text.strip()\n",
    "\n",
    "    #內文移除作者標題發文時間等\n",
    "    for m in metas:\n",
    "        m.extract()\n",
    "    #內文移除看板資訊\n",
    "    for m in main_content.find_all('div', class_='article-metaline-right'):\n",
    "        m.extract()\n",
    "\n",
    "    #ip\n",
    "    IP = main_content.find_all('span', class_='f2')\n",
    "    for i in IP:\n",
    "        if u'發信站' in i.text:\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', i.text).group()\n",
    "    #推文\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for push in pushes:\n",
    "        #main_contain移除push\n",
    "        push.extract()\n",
    "        if not push.find('span', class_='push-tag'):\n",
    "            continue\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "\n",
    "        push_tag = push.find('span', class_='push-tag').text.strip()\n",
    "        push_userid = push.find('span', class_='push-userid').text.strip()\n",
    "        push_content = push.find('span', class_='push-content').text.strip()\n",
    "        push_ipdatetime = push.find('span', class_='push-ipdatetime').text.strip()\n",
    "\n",
    "        messages.append({\n",
    "            'push_tag':push_tag,\n",
    "            'push_userid':push_userid,\n",
    "            'push_content':push_content,\n",
    "            'push_ipdatetime':push_ipdatetime\n",
    "        })\n",
    "\n",
    "        if push_tag == '推':\n",
    "            p += 1\n",
    "        elif push_tag =='噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "    messages_count = {'all': p+b+n, 'count': p-b, 'push':p, 'boo':b, 'neutral':n }\n",
    "\n",
    "    #分割※，取出主文\n",
    "    content = main_content.text.split('※')[0]\n",
    "\n",
    "    data =[{\n",
    "        'url':url,\n",
    "        'article_author':author,\n",
    "        'article_title':title,\n",
    "        'article_date':date,\n",
    "        'article_content':content,\n",
    "        'ip':ip,\n",
    "        'messages_count':messages_count,\n",
    "        'messages':messages    \n",
    "    }]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judyh\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\judyh\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[問卦] 八卦台男下限可以有多低？ - https://www.ptt.cc/bbs/Gossiping/M.1588475805.A.559.html\n",
      "[問卦] 為什麼RG脈衝鋼蛋沒有附贈巨劍啊? - https://www.ptt.cc/bbs/Gossiping/M.1588475831.A.994.html\n",
      "[爆卦] 南韓軍方監視哨站遭北韓槍彈擊中 - https://www.ptt.cc/bbs/Gossiping/M.1588475914.A.1B4.html\n",
      "[問卦] 「含扣」到底是什麼意思 - https://www.ptt.cc/bbs/Gossiping/M.1588475925.A.D28.html\n",
      "Re: [問卦] 不買房可以輕鬆多少 - https://www.ptt.cc/bbs/Gossiping/M.1588475929.A.534.html\n",
      "[問卦] 去一次菜市場花一千算正常的八卦 - https://www.ptt.cc/bbs/Gossiping/M.1588475934.A.9A3.html\n",
      "[問卦] 高中是不是該把國文廢掉改上權力分立總論 - https://www.ptt.cc/bbs/Gossiping/M.1588475949.A.B50.html\n",
      "[新聞]【一線採訪】武漢軍運村變方艙 業主抗議 - https://www.ptt.cc/bbs/Gossiping/M.1588475979.A.417.html\n",
      "Re: [爆卦] 判決上網了！ - https://www.ptt.cc/bbs/Gossiping/M.1588475983.A.603.html\n",
      "[問卦] 矮子練壯是不是更顯矮？ - https://www.ptt.cc/bbs/Gossiping/M.1588475992.A.899.html\n",
      "Re: [新聞] PTT預告「去校門口對小蘿莉動手」 工程師 - https://www.ptt.cc/bbs/Gossiping/M.1588476022.A.776.html\n",
      "[問卦] 欸欸，大甲人給我進來 - https://www.ptt.cc/bbs/Gossiping/M.1588476119.A.424.html\n",
      "Re: [新聞] 轟台版打拋肉「噁到不行」！2配料激怒他 - https://www.ptt.cc/bbs/Gossiping/M.1588476163.A.3FA.html\n",
      "Reach the last article\n"
     ]
    }
   ],
   "source": [
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.find_all('div'):\n",
    "    if 'r-list-sep' in div.get('class'):\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    if 'r-ent' in div.get('class'):\n",
    "        article_title = div.find('div', class_='title').text.strip()\n",
    "        article_URL = 'https://www.ptt.cc' + div.find('a')['href']\n",
    "        print('{} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "        parse_data = crawl_article(article_URL)\n",
    "        \n",
    "        # 將爬完的資料儲存\n",
    "        all_data.append(parse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將爬完的資訊存成 json 檔案\n",
    "with open('parse_data.json', 'w+', encoding = 'utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
